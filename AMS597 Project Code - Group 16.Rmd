---
title: "AMS597 Project - Driven to Predict: Racing meets Statistics"
author: "Shubham Kishore Kale, Kashish Deepak Lalwani, Parth Satish Chavan, Naman Deep"
#output: pdf_document
output:
  pdf_document:
    latex_engine: xelatex

---
### Load Libraries

```{r, warning=FALSE, message=FALSE}
# List of required packages
required_packages <- c(
  "dplyr", "tidyr", "lubridate", "caret", "xgboost", "Matrix", 
  "ggplot2", "ggthemes", "cowplot", "tidyverse", "readr", 
  "cluster", "factoextra", "knitr", "kableExtra", "stats", "vcd", "binom"
)

# Function to install and load a package with full output suppression
install_and_load <- function(pkg) {
  if (!require(pkg, character.only = TRUE)) {
    invisible(capture.output(
      suppressMessages(suppressWarnings(install.packages(pkg, dependencies = TRUE)))
    ))
  }
  invisible(capture.output(
    suppressMessages(suppressWarnings(library(pkg, character.only = TRUE)))
  ))
}

# Apply to all required packages
invisible(lapply(required_packages, install_and_load))


```

### Load Data

```{r}
# Load datasets
results <- read.csv("results.csv")
drivers <- read.csv("drivers.csv")
constructors <- read.csv("constructors.csv")
races <- read.csv("races.csv")
circuits <- read.csv("circuits.csv")
lap_times <- read.csv("lap_times.csv")
pit_stops <- read.csv("pit_stops.csv")
```

# Research Question 1: Lap time prediction

## Using historical lap times, circuit characteristics (layout, length, altitude), and driver performance data (qualifying positions, constructor, driver standings, etc.), build a predictive model to estimate a driverâ€™s lap time for each lap of a race.

```{r}
# Merge all necessary datasets
df <- lap_times %>%
  select(raceId, driverId, lap, milliseconds) %>%
  left_join(results %>% select(raceId, driverId, constructorId, grid),
            by = c("raceId", "driverId")) %>%
  left_join(drivers %>% select(driverId, dob), by = "driverId") %>%
  left_join(constructors %>% select(constructorId, constructorRef), by = "constructorId") %>%
  left_join(races %>% select(raceId, year, round, circuitId, date, time),
            by = "raceId") %>%
  left_join(circuits %>% select(circuitId, circuitRef, lat, lng, alt),
            by = "circuitId") %>%
  left_join(pit_stops %>% select(raceId, driverId, lap, stop, milliseconds),
            by = c("raceId", "driverId", "lap"), suffix = c("", "_pit"))
```

### Data Exploration

```{r, warning=FALSE, message=FALSE}
# Summary of target variable
print("Lap time (Target Variable) summary:")
summary(df$milliseconds)

# Visual: Log transformation justification
ggplot(df %>% filter(milliseconds <= 180000), aes(x = log(milliseconds))) +
  geom_histogram(binwidth = 0.05, fill = "#33a02c", color = "white") +
  labs(title = "Log-Transformed Lap Times", x = "log(Milliseconds)", y = "Count") +
  theme_minimal()

# Missing values
na_summary <- sapply(df, function(x) sum(is.na(x)))
print(na_summary)
```

### Cleaning and Feature Engineering

```{r, warning=FALSE, message=FALSE}
df <- df %>%
  filter(year >= 2019, milliseconds <= 180000) %>%  # Valid upper bound = 3 minutes
  rename(milliseconds_lap = milliseconds) %>%
  mutate(
    dob = ymd(dob),
    race_date = ymd(date),
    race_time = hms(time),
    race_hour = hour(race_time),
    day_of_week = wday(race_date, label = TRUE),
    driver_age = as.numeric(difftime(race_date, dob, units = "days")) / 365.25,
    is_pit_lap = !is.na(milliseconds_pit),
    milliseconds_pit = ifelse(is.na(milliseconds_pit), 0, milliseconds_pit),
    stop = ifelse(is.na(stop), 0, stop),
    grid = ifelse(is.na(grid), 0, grid)
  )

# Lap context
df <- df %>%
  arrange(raceId, driverId, lap) %>%
  group_by(raceId, driverId) %>%
  mutate(
    lap_fraction = lap / max(lap),
    cumulative_pit = lag(cumsum(milliseconds_pit), default = 0),
    lap_time_delta = milliseconds_lap - lag(milliseconds_lap, default = milliseconds_lap[1]),
    lap_bin = cut(lap, breaks = c(0, 15, 35, Inf), labels = c("early", "mid", "late"))
  ) %>% ungroup()

# Global driver average
global_driver_avg <- df %>%
  group_by(driverId) %>%
  summarize(global_avg_lap = mean(milliseconds_lap, na.rm = TRUE))

df <- df %>%
  left_join(global_driver_avg, by = "driverId")

df <- df %>%
  arrange(raceId, driverId, lap) %>%
  group_by(raceId, driverId) %>%
  mutate(
    driver_avg_to_lap = lag(cummean(milliseconds_lap), default = first(global_avg_lap)),
    driver_delta = milliseconds_lap - driver_avg_to_lap
  ) %>% ungroup()

# Constructor average
constructor_avg <- df %>%
  group_by(constructorId) %>%
  summarize(constructor_avg_lap = mean(milliseconds_lap, na.rm = TRUE))

df <- df %>% left_join(constructor_avg, by = "constructorId")
```

### EDA Visuals

```{r, warning=FALSE, message=FALSE}
# Boxplot of lap times by constructor
ggplot(df, aes(x = reorder(constructorRef, milliseconds_lap, median), y = milliseconds_lap)) +
  geom_boxplot(fill = "#4dac26") +
  coord_flip() +
  labs(title = "Lap Time by Constructor", x = "Constructor", y = "Lap Time (ms)") +
  theme_minimal()

# Driver age vs lap time
ggplot(df, aes(x = driver_age, y = milliseconds_lap)) +
  geom_point(alpha = 0.2, color = "#045a8d") +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  labs(title = "Driver Age vs Lap Time", x = "Driver Age (Years)", y = "Lap Time (ms)") +
  theme_minimal()
```

###  Final Modeling Dataset

```{r, warning=FALSE, message=FALSE}
model_df <- df %>%
  filter(!is_pit_lap) %>%
  select(milliseconds_lap, year, grid, lap, lap_fraction, cumulative_pit,
         lat, lng, alt, race_hour, day_of_week, driver_age,
         driver_avg_to_lap, constructor_avg_lap, driver_delta,
         lap_time_delta, lap_bin,
         driverId, constructorId, circuitRef) %>%
  drop_na()

model_df <- model_df %>%
  mutate(across(c(driverId, constructorId, circuitRef, day_of_week, lap_bin), as.factor)) %>%
  mutate(log_lap_time = log(milliseconds_lap))

# Train-test split
train_data <- model_df %>% filter(year %in% 2019:2023)
test_data  <- model_df %>% filter(year == 2024)

# Outlier clipping (even after filtering)
q_lo <- quantile(train_data$log_lap_time, 0.01)
q_hi <- quantile(train_data$log_lap_time, 0.99)
train_data <- train_data %>% filter(log_lap_time >= q_lo, log_lap_time <= q_hi)

# Visual explanation for clipping
ggplot(train_data, aes(x = log_lap_time)) +
  geom_histogram(fill = "#ff7f00", color = "white", bins = 30) +
  geom_vline(xintercept = c(q_lo, q_hi), color = "red", linetype = "dashed") +
  labs(title = "Clipping Outliers on log(lap_time)", x = "log(milliseconds)", y = "Count") +
  theme_minimal()
```

### Encoding for Models

```{r, warning=FALSE, message=FALSE}
encode_for_xgb <- function(df) {
  df_num <- df %>% select(-milliseconds_lap, -year)
  for (col in names(df_num)) {
    if (is.factor(df_num[[col]])) {
      df_num[[col]] <- as.numeric(as.factor(df_num[[col]]))
    }
  }
  return(df_num)
}

train_mat <- encode_for_xgb(train_data)
test_mat <- encode_for_xgb(test_data)
```

### Baseline: Linear Regression

```{r, warning=FALSE, message=FALSE}
lm_model <- lm(log_lap_time ~ ., data = train_mat)
pred_log_lm <- predict(lm_model, newdata = test_mat)
pred_ms_lm <- exp(pred_log_lm)
rmse_lm <- RMSE(pred_ms_lm, test_data$milliseconds_lap)

# Plot predictions
ggplot(data.frame(Actual = test_data$milliseconds_lap, Predicted = pred_ms_lm),
       aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.4, color = "#33a02c") +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Linear Regression: Actual vs Predicted", x = "Actual (ms)", y = "Predicted (ms)") +
  theme_minimal()
```

### XGBoost Regression

```{r, warning=FALSE, message=FALSE}
dtrain <- xgb.DMatrix(data = as.matrix(train_mat %>% select(-log_lap_time)),
                      label = train_mat$log_lap_time)
dtest <- xgb.DMatrix(data = as.matrix(test_mat %>% select(-log_lap_time)))

params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  eta = 0.03,
  max_depth = 6,
  subsample = 0.8,
  colsample_bytree = 0.8,
  min_child_weight = 5,
  gamma = 0.1
)

set.seed(42)
cv <- xgb.cv(params = params, data = dtrain, nrounds = 300,
             nfold = 5, early_stopping_rounds = 15, verbose = 0)

best_nrounds <- cv$best_iteration
final_model_xgb <- xgb.train(params = params, data = dtrain, nrounds = best_nrounds)

pred_log_xgb <- predict(final_model_xgb, dtest)
pred_ms_xgb <- exp(pred_log_xgb)
rmse_xgb <- RMSE(pred_ms_xgb, test_data$milliseconds_lap)

# XGBoost Prediction Plot
ggplot(data.frame(Actual = test_data$milliseconds_lap, Predicted = pred_ms_xgb),
       aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.4, color = "#1f78b4") +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "XGBoost: Actual vs Predicted", x = "Actual (ms)", y = "Predicted (ms)") +
  theme_minimal()
```

### Model Comparison

```{r, warning=FALSE, message=FALSE}
cat("Model Performance (RMSE):\n")
cat("Linear Regression: ", round(rmse_lm, 2), "\n")
cat("XGBoost         : ", round(rmse_xgb, 2), "\n")
```

# Research Question 2: Clustering Drivers by Racing Style

## How can data-driven methods methods be used to identify different types of Formula 1 drivers based on their race performance, and what do these groupings reveal about common racing styles and strategies?

### Feature Engineering

```{r}
# Merge lap_times with races to add race details
races <- races %>%filter(year > 2010)
lap_races <- lap_times %>%
  inner_join(races, by = "raceId")

# Calculate average lap time and its standard deviation for each driver
lap_summary <- lap_races %>%
  group_by(driverId) %>%
  summarise(avg_lap_time = mean(milliseconds, na.rm = TRUE),
            sd_lap_time  = sd(milliseconds, na.rm = TRUE))

# Calculate pit stop frequency: average number of pit stops per race for each driver
pit_summary <- pit_stops %>%
  group_by(driverId, raceId) %>%
  summarise(num_pit_stops = n(), .groups = "drop") %>%
  group_by(driverId) %>%
  summarise(avg_pit_stops = mean(num_pit_stops, na.rm = TRUE))

# Calculate overtaking statistics: positions gained = grid position minus finishing position
overtake_summary <- results %>%
  mutate(positions_gained = grid - positionOrder) %>%
  group_by(driverId) %>%
  summarise(avg_positions_gained = mean(positions_gained, na.rm = TRUE))

# Merge all summaries with driver details
driver_features <- drivers %>%
  select(driverId, forename, surname, nationality) %>%
  inner_join(lap_summary, by = "driverId") %>%
  inner_join(pit_summary, by = "driverId") %>%
  inner_join(overtake_summary, by = "driverId") %>%
  # Create a single name column for clarity
  mutate(driver_name = paste(forename, surname))%>%
  select(-forename, -surname)

```

### EDA

```{r}
# Histogram for Average Lap Time
ggplot(driver_features, aes(x = avg_lap_time)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Distribution of Average Lap Time", x = "Average Lap Time (ms)", y = "Frequency")

# Histogram for Average Pit Stop Frequency
ggplot(driver_features, aes(x = avg_pit_stops)) +
  geom_histogram(bins = 30, fill = "darkgreen", color = "black") +
  labs(title = "Distribution of Average Pit Stops", x = "Average Pit Stops per Race", y = "Frequency")

# Histogram for Average Positions Gained (Overtaking Metric)
ggplot(driver_features, aes(x = avg_positions_gained)) +
  geom_histogram(bins = 30, fill = "darkred", color = "black") +
  labs(title = "Distribution of Average Positions Gained", x = "Average Positions Gained", y = "Frequency")

```
Observations:

1. Average Lap Time

Distribution shape: Left-skewed.

Range: ~70,000 ms to 120,000 ms.

Insight: Most drivers cluster around 90,000â€“100,000 ms; very few extremely fast or slow drivers exist.
Outliers: Some drivers have extremely low average lap times (e.g., ~72,000 ms) likely due to partial race data or anomalies.

2. Average Pit Stops

Distribution shape: Bell-like with mild skew.

Range: ~1.5 to ~3 pit stops per race.

Insight: Majority of drivers average around 2â€“2.5 pit stops, suggesting a common race strategy; a few edge cases have very high pit stops.

3. Average Positions Gained

Distribution shape: Roughly centered, with both positive and negative values.

Range: ~-2.5 to +5 positions gained per race.

Insight: Most drivers hover around 0, indicating minimal positional change, but some consistently overtake more than 3â€“4 cars per race â€” suggesting aggressive racecraft.



## K-means Clustering

```{r}
# Select features for clustering analysis
features <- driver_features %>%
  select(avg_lap_time, sd_lap_time, avg_pit_stops, avg_positions_gained)

# Standardize features (zero mean, unit variance)
features_scaled <- scale(features)

```

### Finding Optimal K value for clustering - Elbow Method

```{r}
# Using the Elbow Method to identify the appropriate number of clusters
wss <- numeric(15)
for (k in 2:15) {
  set.seed(123)
  kmeans_model <- kmeans(features_scaled, centers = k, nstart = 50,iter.max = 100)
  wss[k] <- kmeans_model$tot.withinss
}
# Plot the Elbow Curve
plot(2:15, wss[2:15], type = "b", pch = 19,
     xlab = "Number of Clusters (K)", ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method for Optimal K")

for (y in wss[2:15]) {
  abline(h = y, lty = "dashed", col = "gray")
}

abline(h = wss[5], lty = "dashed", col = "red", lwd = 2)
abline(v = 5, lty = "dashed", col = "green", lwd = 2)
```
Observation:

Optimal Value of K - Method Used: Elbow Method

Plot: WSS (Within-Cluster Sum of Squares) vs. number of clusters k

A visible elbow point appears at k=5, where the reduction in WSS starts flattening.

A red dashed horizontal line at WSS for K = 5 
and a green vertical line at k=5 visually emphasize this optimal point.

Hence chosen k=5 based on the Elbow Method's inflection point.

### Cluster Validation and Visualisation

```{r}

# Perform K-Means clustering with k = 5 clusters
set.seed(123)
kmeans_result <- kmeans(features_scaled, centers = 5, nstart = 25, iter.max = 100)

driver_features$cluster <- as.factor(kmeans_result$cluster)

sil <- silhouette(kmeans_result$cluster, dist(features_scaled))
mean_sil_width <- mean(sil[, 3])

cat("Average Silhouette Score for k = 5:", mean_sil_width, "\n")

print(fviz_silhouette(sil) +
  labs(title = "Silhouette Plot for K-Means Clustering (k = 5)"))

# Visualize the clustering result using PCA for dimensionality reduction
fviz_cluster(kmeans_result, data = features_scaled, 
             ellipse.type = "convex",
             labelsize = 6,
             main = "K-Means Clustering of F1 Drivers")



```

```{r}
# Cluster Characteristics describing each cluster
cluster_characteristics <- driver_features %>%
  group_by(cluster) %>%
  summarise(
    count = n(),
    mean_avg_lap_time = mean(avg_lap_time, na.rm = TRUE),
    mean_sd_lap_time = mean(sd_lap_time, na.rm = TRUE),
    mean_avg_pit_stops = mean(avg_pit_stops, na.rm = TRUE),
    mean_positions_gained = mean(avg_positions_gained, na.rm = TRUE)
  )

kable(cluster_characteristics,digits = 3, caption = "Cluster Characteristics")%>%
  kable_styling(font_size = 8)

kable(driver_features,digits = 3, caption = "Driver-Level Features")%>%
kable_styling(font_size = 8)


```
### Interpreting F1 Driver Archetypes from Clustering and Silhouette Scores

The K-means clustering revealed five distinct groups of Formula 1 drivers based on their average lap time, lap time consistency, pit stop frequency, and positions gained per race. Below is a breakdown of each cluster, blending both performance characteristics and silhouette-based clustering quality:

#### Cluster 1: Efficient Overtakers


Size: 14 drivers

Avg. Silhouette Width: 0.27

Traits:

  Solid average lap times.

  Slightly above-average pit stop usage.

Consistently gain positions during races, reflecting assertive on-track movement.

Interpretation:
These drivers are effective at overtaking and making progress during races. However, the lower silhouette score suggests they share similarities with other clusters, likely due to mixed strategies or team dynamics, resulting in less clean separation.

#### Cluster 2: Short Run Strategists


Size: 2 drivers

Avg. Silhouette Width: 0.47 (highest)

Traits:

  Exceptionally fast lap times, the quickest among all clusters.

  High pit stop frequency, possibly from sprint races or partial data.

Interpretation:
Despite the small size, this cluster shows strong cohesion and clear distinction from others. These drivers are likely outliers or specialists with high performance in short runs. The high silhouette score confirms they are well-separated and internally consistent.

#### Cluster 3: Mid-Pack Movers


Size: 6 drivers

Avg. Silhouette Width: 0.22 (lowest)

Traits:

  Low pit stop counts.

  Tight lap time consistency and positive position gains.

Interpretation:
These drivers are strategic and efficient, making progress during races while minimizing variability. However, the low silhouette score indicates that they overlap with other driver types, possibly because of their balanced traits that straddle both aggressive and conservative styles.

#### Cluster 4: Baseline Finishers


Size: 39 drivers (largest group)

Avg. Silhouette Width: 0.45

Traits:

  Slightly negative positions gained â†’ drivers tend to lose positions during races.

  Lap times and pit stop behavior suggest average, consistent performance.

Interpretation:
This is the most cohesive and well-defined cluster, forming the core baseline of the F1 driver pool. The high silhouette score validates this group as a well-separated, stable archetypeâ€”likely made up of regular midfield or slightly underperforming drivers.

#### Cluster 5: Slower but Stable


Size: 15 drivers

Avg. Silhouette Width: 0.32

Traits:

  Slowest average lap times and highest lap time variability.

  Despite this, pit stop frequency remains moderate.

Interpretation:
These drivers may represent rookies, backmarkers, or technically constrained cars. They are somewhat consistent in strategy but underperform on pace. The moderate silhouette score suggests internal cohesion with some overlap with neighboring clusters.

# Research Question 3: Impact of Grid Position on Race Outcome
## In Formula 1, the starting grid position (result of qualifying) is often considered crucial to race success. We want to statistically test whether starting in the Top 5 on the grid significantly increases the chances of winning compared to starting from a Lower Position.

### Data Preprocessing

```{r}

# Merge datasets by raceId to include year
data <- merge(results, races[, c("raceId", "year")], by = "raceId")

data_clean <- data %>%
  filter(!is.na(grid), !is.na(positionOrder), positionOrder > 0) %>%
  mutate(
    # Create binary variable for starting grid group
    GridGroup = ifelse(grid <= 5, "Top 5", "Lower"),
    # Create binary variable for race outcome
    Win = ifelse(positionOrder == 1, "Win", "Not Win"),
    # Podium finish: position 1 to 3
    PodiumFinish = ifelse(positionOrder <= 3, "Yes", "No"),
    # Front row: grid 1 or 2
    FrontRowStart = ifelse(grid <= 2, "Yes", "No"),
    # Midfield: grid 6â€“10
    MidfieldStart = ifelse(grid >= 6 & grid <= 10, "Yes", "No")
  )
```

### Contingency Table

```{r}

# This table compares Top 5 vs. Lower starters across Win/Not Win
table_result <- table(data_clean$GridGroup, data_clean$Win)
print(table_result)
```

###  Chi-Square Test

```{r}


chisq_test <- chisq.test(table_result)

# Print expected counts for assumption check
chisq_test$expected

chisq_test <- chisq.test(table_result)
print(chisq_test)

expected <- chisq_test$expected

# CramÃ©râ€™s V to measure effect size
n <- sum(table_result)
k <- min(nrow(table_result), ncol(table_result))
cramers_v <- sqrt(chisq_test$statistic / (n * (k - 1)))
print(cramers_v)
```




### Win Rate & Confidence Interval

```{r}
# Calculate and compare Wilson confidence intervals for win rates of Top 5 vs Lower grid starters

top5 <- data_clean %>% filter(GridGroup == "Top 5")
lower <- data_clean %>% filter(GridGroup == "Lower")

top5_win_rate <- binom.confint(sum(top5$Win == "Win"), nrow(top5), method = "wilson")
lower_win_rate <- binom.confint(sum(lower$Win == "Win"), nrow(lower), method = "wilson")

print(top5_win_rate)
print(lower_win_rate)
```



```{r}
# Compute win rates by grid group and visualize the comparison using a bar plot

win_summary <- data_clean %>%
  group_by(GridGroup) %>%
  summarise(WinRate = mean(Win == "Win"))

ggplot(win_summary, aes(x = GridGroup, y = WinRate, fill = GridGroup)) +
  geom_bar(stat = "identity", width = 0.6, color = "black") +
  geom_text(aes(label = scales::percent(WinRate, accuracy = 0.1)),
            vjust = -0.5, size = 4.5, fontface = "bold") +
  labs(title = "Win Rate by Grid Position Group",
       y = "Win Proportion",
       x = "Grid Group") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1),
                     expand = expansion(mult = c(0, 0.1))) +
  scale_fill_manual(values = c("Top 5" = "#0072B2", "Lower" = "#E69F00")) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title = element_text(),
        legend.position = "none")
```


### Observed vs Expected

```{r}

# Visualize observed vs. expected race outcomes by grid group 
# using a grouped bar plot with expected values overlaid

observed <- as.data.frame(table_result)
expected_df <- as.data.frame(as.table(expected))
colnames(observed) <- c("GridGroup", "Win", "Observed")
colnames(expected_df) <- c("GridGroup", "Win", "Expected")
obs_exp <- merge(observed, expected_df, by = c("GridGroup", "Win"))

ggplot(obs_exp, aes(x = GridGroup, y = Observed, fill = Win)) +
  geom_bar(stat = "identity", position = position_dodge(0.8), 
           width = 0.6, color = "black") +
  geom_point(aes(y = Expected), color = "red", shape = 18, size = 4,
             position = position_dodge(width = 0.8)) +
  labs(title = "Observed vs Expected Race Outcomes by Grid Group",
       x = "Grid Group", y = "Count", fill = "Race Outcome") +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title = element_text()) +
  scale_fill_manual(values = c("Win" = "#56B4E9", "Not Win" = "#D55E00"))
```
In the "Top 5" grid group:
  
    - The observed number of wins** is significantly higher than expected under independence.
    - The red point (expected value) is much lower than the bar (actual count).

 In the "Lower" grid group:
    
    - The observed number of wins is much lower than expected.
    - The red point (expected count) overshoots the bar, indicating fewer wins than predicted.




### Podiums vs Wins

```{r}
# Summarize key race statistics by grid group

summary_df <- data_clean %>%
  group_by(GridGroup) %>%
  summarise(
    Total = n(),
    Wins = sum(Win == "Win"),
    Podiums = sum(PodiumFinish == "Yes"),
    FrontRowStarts = sum(FrontRowStart == "Yes"),
    MidfieldStarts = sum(MidfieldStart == "Yes")
  )
print(summary_df)

# Visualize comparison of Wins vs Podiums using grouped bar plot

ggplot(summary_df, aes(x = GridGroup)) +
  geom_bar(aes(y = Wins, fill = "Wins"), stat = "identity", 
           position = position_dodge(width = 0.7),
           color = "black", width = 0.6) +
  geom_bar(aes(y = Podiums, fill = "Podiums"), stat = "identity", 
           position = position_dodge(width = 0.7),
           color = "black", width = 0.6) +
  scale_fill_manual(name = "Type", values = c("Wins" = "#0072B2", "Podiums" = "#56B4E9")) +
  labs(title = "Wins vs Podiums by Grid Group", y = "Count", x = "Grid Group") +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(hjust = 0.5))
```

 

### Grid Position Distribution of Winners

```{r}
# Filter winning drivers and plot a histogram of their starting grid positions

winner_grid <- data_clean %>% filter(Win == "Win")

ggplot(winner_grid, aes(x = grid)) +
  geom_histogram(binwidth = 1, fill = "#009E73", color = "black") +
  labs(title = "Grid Positions for Winning Drivers", 
       x = "Grid Position", y = "Number of Wins") +
  scale_x_continuous(breaks = seq(1, max(winner_grid$grid), by = 2)) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5))

```

Final Interpretation:

1. The Chi-square test assesses whether there is a significant association between grid group and win outcome.
   CramÃ©râ€™s V â‰ˆ 0.34, indicating a moderate effect size. Together, these results confirm that starting position has a         meaningful impact on race outcomes in Formula 1.

2. The calculation uses the Wilson method to estimate 95% confidence intervals for win rates.

   Top 5 starters have an average win rate of 17.5% with a 95% CI of [15.68%, 19.51%].

   Lower grid starters have a win rate of 0.47% with a 95% CI of [0.30%, 0.71%].

3. Drivers starting in the top postitions have an average win rate of 17.5%, while those starting in Lower 
   positions have a dramatically lower win rate of just 0.47%.

   This means that drivers in the Top 5 are more than 22 times more likely to win
   than those starting from P6 or lower.


